{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer les packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, Bidirectional, GRU, Dropout\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet df\n",
      "   label                                              tweet\n",
      "0      1  @user homegrown rightie white americans are 10...\n",
      "1      1  @user @user  is this a new one? either way, #m...\n",
      "2      0  i am thankful for pillows. #thankful #positive...\n",
      "3      0  it's unfounate today that aids is under contro...\n",
      "4      0      i am super. #i_am #positive #affirmation     \n",
      "\n",
      "------è-------\n",
      "\n",
      "Tweets df\n",
      "  coachella_sentiment                                               text\n",
      "0            positive  #Coachella2015 tickets selling out in less tha...\n",
      "1            positive  RT @sudsybuddy: WAIT THIS IS ABSOLUTE FIRE _ÙÓ...\n",
      "2            positive  #Coachella2015 #VIP passes secured! See you th...\n",
      "3            positive  PhillyÛªs @warondrugsjams will play #Coachell...\n",
      "4            positive  If briana and her mom out to #Coachella2015  i...\n"
     ]
    }
   ],
   "source": [
    "# Importer les données\n",
    "path1 = './data/tweet_data.csv'\n",
    "path2 = './data/Tweets_data.csv'\n",
    "\n",
    "df_tweet = pd.read_csv(path1)\n",
    "df_Tweets = pd.read_csv(path2)\n",
    "\n",
    "# Afficher les données\n",
    "print('tweet df')\n",
    "print(df_tweet.head())\n",
    "\n",
    "print('\\n------è-------\\n')\n",
    "\n",
    "print('Tweets df')\n",
    "print(df_Tweets.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concaténation des deux tweet_data dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uniformisons les labels en (label et tweet) ...\n",
    "df_Tweets.columns = ['label', 'tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selectionnons uniquement les lignes positive et négative de df_Tweets ...\n",
    "data = []\n",
    "for i in range(df_Tweets.shape[0]):\n",
    "    label = df_Tweets[\"label\"][i]\n",
    "    tweet = df_Tweets[\"tweet\"][i]\n",
    "    \n",
    "    if(label == \"positive\" or label == \"negative\"):\n",
    "        label = 0 if label == \"positive\" else 1\n",
    "        data.append({'label': label, 'tweet': tweet})\n",
    "    \n",
    "df_Tweets = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet df\n",
      "   label                                              tweet\n",
      "0      1  @user homegrown rightie white americans are 10...\n",
      "1      1  @user @user  is this a new one? either way, #m...\n",
      "2      0  i am thankful for pillows. #thankful #positive...\n",
      "3      0  it's unfounate today that aids is under contro...\n",
      "4      0      i am super. #i_am #positive #affirmation     \n",
      "\n",
      "------è-------\n",
      "\n",
      "Tweets df\n",
      "   label                                              tweet\n",
      "0      0  #Coachella2015 tickets selling out in less tha...\n",
      "1      0  RT @sudsybuddy: WAIT THIS IS ABSOLUTE FIRE _ÙÓ...\n",
      "2      0  #Coachella2015 #VIP passes secured! See you th...\n",
      "3      0  PhillyÛªs @warondrugsjams will play #Coachell...\n",
      "4      0  If briana and her mom out to #Coachella2015  i...\n"
     ]
    }
   ],
   "source": [
    "# Affichons de nouveau les données des deux df\n",
    "print('tweet df')\n",
    "print(df_tweet.head())\n",
    "\n",
    "print('\\n------è-------\\n')\n",
    "\n",
    "print('Tweets df')\n",
    "print(df_Tweets.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concaténons les deux data frame en un seul ...\n",
    "df = pd.concat([df_tweet, df_Tweets])\n",
    "df.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Coachella2015 tickets selling out in less than 40 minutes _Ù_¦_Ù___Ù___Ù÷_ÙÎµ_ÙÎµ_Ù___Ù_¦ http://t.co/SmoXyteIMJ\n"
     ]
    }
   ],
   "source": [
    "print(df['tweet'][4484])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nettoyage du df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in c:\\users\\morcodou.seck\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.1.73)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in c:\\users\\morcodou.seck\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: anyascii in c:\\users\\morcodou.seck\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
      "Requirement already satisfied: pyahocorasick in c:\\users\\morcodou.seck\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from textsearch>=0.0.21->contractions) (2.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nettoyage des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemma = WordNetLemmatizer()\n",
    "Stopwords = stopwords.words('english')\n",
    "\n",
    "def cleannig_tweet(tweet):\n",
    "    expanded_all = []\n",
    "    tweet = re.sub(r'http[s]*:?//\\S+','', tweet)\n",
    "    tweet = re.sub(r'@[\\w\\-\\.]+', '', tweet)\n",
    "    tweet = re.sub(r'[\\w\\-\\.]+@[\\w\\-\\.]+', '', tweet)\n",
    "    tweet = re.sub(r'&\\w+','', tweet)\n",
    "    tweet = re.sub(r'[^a-zA-Z\\s]+', ' ', tweet)\n",
    "    tweet = re.sub(r'^\\s|\\s$', '', tweet)\n",
    "    tweet = re.sub(r'\\s{2,}', ' ', tweet).lower()\n",
    "    \n",
    "    \n",
    "    for word in tweet.split():\n",
    "        expanded_all.append(contractions.fix(word))\n",
    "    tweet = ' '.join(expanded_all)\n",
    "    \n",
    "    \n",
    "    tweet = ' '.join([word for word in tweet.split() if word not in Stopwords])\n",
    "\n",
    "    tweet = [lemma.lemmatize(word) for word in tweet.split()]\n",
    "    tweet = \" \".join(tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appliquer la fonction cleannig_tweet\n",
    "df['new_tweet'] = df.tweet.apply(func = cleannig_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Coachella2015 tickets selling out in less than 40 minutes _Ù_¦_Ù___Ù___Ù÷_ÙÎµ_ÙÎµ_Ù___Ù_¦ http://t.co/SmoXyteIMJ\n",
      "coachella ticket selling le minute\n"
     ]
    }
   ],
   "source": [
    "print(df['tweet'][4484])\n",
    "print(df['new_tweet'][4484])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>new_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>@user homegrown rightie white americans are 10...</td>\n",
       "      <td>homegrown rightie white american time likely h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>@user @user  is this a new one? either way, #m...</td>\n",
       "      <td>new one either way men disgraced harassment ve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>i am thankful for pillows. #thankful #positive...</td>\n",
       "      <td>thankful pillow thankful positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>it's unfounate today that aids is under contro...</td>\n",
       "      <td>unfounate today aid control hate kill gay year...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>i am super. #i_am #positive #affirmation</td>\n",
       "      <td>super positive affirmation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                              tweet  \\\n",
       "0      1  @user homegrown rightie white americans are 10...   \n",
       "1      1  @user @user  is this a new one? either way, #m...   \n",
       "2      0  i am thankful for pillows. #thankful #positive...   \n",
       "3      0  it's unfounate today that aids is under contro...   \n",
       "4      0      i am super. #i_am #positive #affirmation        \n",
       "\n",
       "                                           new_tweet  \n",
       "0  homegrown rightie white american time likely h...  \n",
       "1  new one either way men disgraced harassment ve...  \n",
       "2                  thankful pillow thankful positive  \n",
       "3  unfounate today aid control hate kill gay year...  \n",
       "4                         super positive affirmation  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Affichons le jeu de données\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vectorisation des tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Vectorison avec TfidfVectorizer\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# # Initialisation\n",
    "# vect = TfidfVectorizer()\n",
    "\n",
    "# # Matrice des tweets\n",
    "# vect_matrix = vect.fit_transform(df['new_tweet'])\n",
    "\n",
    "# vect_array = vect_matrix.toarray()\n",
    "\n",
    "# nbr_word_unique = len(vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorison avec Tokenizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Nombre de mots\n",
    "nbr_word_unique = len(set(\" \".join(df.new_tweet).split()))\n",
    "\n",
    "# Initialiser le modèle Tokenizer\n",
    "tokenizer = Tokenizer(num_words = nbr_word_unique, split=' ')\n",
    "\n",
    "# Entrainer les données\n",
    "tokenizer.fit_on_texts(df['new_tweet'].values)\n",
    "\n",
    "# Vectoriser\n",
    "vect_array = tokenizer.texts_to_sequences(df['new_tweet'].values)\n",
    "\n",
    "# Padding\n",
    "vect_array = pad_sequences(vect_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder le tokenizer en JSON\n",
    "tokenizer_json = tokenizer.to_json()\n",
    "with open('english_tokenizer.json', 'w', encoding='utf-8') as json_file:\n",
    "    json_file.write(tokenizer_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11229"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Le nombre de mot uniques dans les tweets\n",
    "nbr_word_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Définission des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Installons keras-tuner\n",
    "!pip install -q -U keras-tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Séparation des données en train, test et validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Labels\n",
    "y = df.label.values\n",
    "\n",
    "# Splitter en train et test\n",
    "x_train, x_test, y_train, y_test = train_test_split(vect_array, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Splitter validation et test\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définissons l'opitmizer\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bidir_lstm_model_builder(hp):\n",
    "    emb_dimention = hp.Int('emb_dimention', min_value=32, max_value=512, step=32)\n",
    "    nbr_neurones = hp.Int('nbr_neurones', min_value=32, max_value=512, step=32)\n",
    "    nbr_neurones_l2 = hp.Int('nbr_neurones', min_value=32, max_value=512, step=32)\n",
    "    learning_rate = hp.Choice('learning_rate', values=[0.01, 0.001, 0.0001])\n",
    "    dropout = hp.Boolean(\"dropout\")\n",
    "    \n",
    "    Adam(learning_rate=learning_rate)\n",
    "    \n",
    "    bidir_lstm_model=Sequential()\n",
    "    bidir_lstm_model.add(Embedding(nbr_word_unique, emb_dimention, input_length=vect_array.shape[1]))\n",
    "    bidir_lstm_model.add(Bidirectional(LSTM(nbr_neurones, return_sequences=True)))\n",
    "    if dropout :\n",
    "        bidir_lstm_model.add(Dropout(0.4))\n",
    "    bidir_lstm_model.add(Bidirectional(LSTM(nbr_neurones_l2)))\n",
    "    if dropout: \n",
    "        bidir_lstm_model.add(Dropout(0.4))\n",
    "    bidir_lstm_model.add(Dense(1,activation='sigmoid'))\n",
    "    bidir_lstm_model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    \n",
    "    return bidir_lstm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bidir_gru_model_builder(hp):\n",
    "    emb_dimention = hp.Int('emb_dimention', min_value=32, max_value=512, step=32)\n",
    "    nbr_neurones = hp.Int('nbr_neurones', min_value=32, max_value=512, step=32)\n",
    "    nbr_neurones_l2 = hp.Int('nbr_neurones', min_value=32, max_value=512, step=32)\n",
    "    learning_rate = hp.Choice('learning_rate', values=[0.01, 0.001, 0.0001])\n",
    "    dropout = hp.Boolean(\"dropout\")\n",
    "    \n",
    "    Adam(learning_rate=learning_rate)\n",
    "    \n",
    "    bidir_gru_model=Sequential()\n",
    "    bidir_gru_model.add(Embedding(nbr_word_unique, emb_dimention, input_length=vect_array.shape[1]))\n",
    "    bidir_gru_model.add(Bidirectional(GRU(nbr_neurones, return_sequences=True)))\n",
    "    if dropout :\n",
    "        bidir_gru_model.add(Dropout(0.4))\n",
    "    bidir_gru_model.add(Bidirectional(GRU(nbr_neurones_l2)))\n",
    "    if dropout: \n",
    "        bidir_gru_model.add(Dropout(0.4))\n",
    "    bidir_gru_model.add(Dense(1,activation='sigmoid'))\n",
    "    bidir_gru_model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    \n",
    "    return bidir_gru_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_model_builder(hp):\n",
    "    emb_dimention = hp.Int('emb_dimention', min_value=32, max_value=512, step=32)\n",
    "    nbr_neurones = hp.Int('nbr_neurones', min_value=32, max_value=512, step=32)\n",
    "    nbr_neurones_l2 = hp.Int('nbr_neurones', min_value=32, max_value=512, step=32)\n",
    "    learning_rate = hp.Choice('learning_rate', values=[0.01, 0.001, 0.0001])\n",
    "    dropout = hp.Boolean(\"dropout\")\n",
    "    \n",
    "    Adam(learning_rate=learning_rate)\n",
    "    \n",
    "    lstm_model=Sequential()\n",
    "    lstm_model.add(Embedding(nbr_word_unique, emb_dimention, input_length=vect_array.shape[1]))\n",
    "    lstm_model.add(LSTM(nbr_neurones, return_sequences=True))\n",
    "    if dropout :\n",
    "        lstm_model.add(Dropout(0.4))\n",
    "    lstm_model.add(LSTM(nbr_neurones_l2))\n",
    "    if dropout: \n",
    "        lstm_model.add(Dropout(0.4))\n",
    "    lstm_model.add(Dense(1,activation='sigmoid'))\n",
    "    lstm_model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    \n",
    "    return lstm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru_model_builder(hp):\n",
    "    emb_dimention = hp.Int('emb_dimention', min_value=32, max_value=512, step=32)\n",
    "    nbr_neurones = hp.Int('nbr_neurones', min_value=32, max_value=512, step=32)\n",
    "    nbr_neurones_l2 = hp.Int('nbr_neurones', min_value=32, max_value=512, step=32)\n",
    "    learning_rate = hp.Choice('learning_rate', values=[0.01, 0.001, 0.0001])\n",
    "    dropout = hp.Boolean(\"dropout\")\n",
    "    \n",
    "    Adam(learning_rate=learning_rate)\n",
    "    \n",
    "    gru_model=Sequential()\n",
    "    gru_model.add(Embedding(nbr_word_unique, emb_dimention, input_length=vect_array.shape[1]))\n",
    "    gru_model.add(GRU(nbr_neurones, return_sequences=True))\n",
    "    if dropout :\n",
    "        gru_model.add(Dropout(0.4))\n",
    "    gru_model.add(GRU(nbr_neurones_l2))\n",
    "    if dropout: \n",
    "        gru_model.add(Dropout(0.4))\n",
    "    gru_model.add(Dense(1,activation='sigmoid'))\n",
    "    gru_model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    \n",
    "    return gru_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_model(model_to_tune):\n",
    "    # Mettons en place un rapel d'arrêt après avoir atteint une certaine valeur\n",
    "    stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "    \n",
    "    # Instancions le tuner\n",
    "    tuner = kt.RandomSearch(\n",
    "        model_to_tune,\n",
    "        objective='val_accuracy',\n",
    "        max_trials = 2,\n",
    "        overwrite = True,\n",
    "        directory='tuners_dir',\n",
    "        project_name='intro_to_kt'\n",
    "    )\n",
    "    \n",
    "    # Exécutons la recherche des paramètres\n",
    "    tuner.search(x_train, y_train, validation_data=(x_val, y_val), epochs=3, callbacks=[stop_early])\n",
    "    \n",
    "    return tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2 Complete [00h 04m 25s]\n",
      "val_accuracy: 0.8483606576919556\n",
      "\n",
      "Best val_accuracy So Far: 0.8483606576919556\n",
      "Total elapsed time: 00h 07m 34s\n"
     ]
    }
   ],
   "source": [
    "# Récupérons les models tunes\n",
    "bidir_lstm_tuner = tune_model(bidir_lstm_model_builder)\n",
    "bidir_gru_tuner = tune_model(bidir_gru_model_builder)\n",
    "lstm_tuner = tune_model(lstm_model_builder)\n",
    "gru_tuner = tune_model(gru_model_builder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construisons les modèles\n",
    "bidir_lstm_model = bidir_lstm_tuner.hypermodel.build(bidir_lstm_tuner.get_best_hyperparameters(num_trials=1)[0])\n",
    "\n",
    "bidir_gru_model = bidir_gru_tuner.hypermodel.build(bidir_gru_tuner.get_best_hyperparameters(num_trials=1)[0])\n",
    "\n",
    "lstm_model = lstm_tuner.hypermodel.build(lstm_tuner.get_best_hyperparameters(num_trials=1)[0])\n",
    "\n",
    "gru_model = gru_tuner.hypermodel.build(gru_tuner.get_best_hyperparameters(num_trials=1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    {\"name\": \"bidirectional LSTM\", \"model\": bidir_lstm_model, \"score\": 0},\n",
    "    {\"name\": \"LSTM\", \"model\": lstm_model, \"score\": 0},\n",
    "    {\"name\": \"bidirectional GRU\", \"model\": bidir_gru_model, \"score\": 0},\n",
    "    {\"name\": \"GRU\", \"model\": gru_model, \"score\": 0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bidirectional LSTM\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 21, 224)           2515296   \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 21, 960)           2707200   \n",
      " al)                                                             \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 21, 960)           0         \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirecti  (None, 960)               5533440   \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 960)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 961       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10756897 (41.03 MB)\n",
      "Trainable params: 10756897 (41.03 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "\n",
      "\n",
      "LSTM\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 21, 160)           1796640   \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 21, 480)           1230720   \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 480)               1845120   \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 481       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4872961 (18.59 MB)\n",
      "Trainable params: 4872961 (18.59 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "\n",
      "\n",
      "bidirectional GRU\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 21, 224)           2515296   \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirecti  (None, 21, 576)           888192    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirecti  (None, 576)               1496448   \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 577       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4900513 (18.69 MB)\n",
      "Trainable params: 4900513 (18.69 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "\n",
      "\n",
      "GRU\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, 21, 288)           3233952   \n",
      "                                                                 \n",
      " gru_4 (GRU)                 (None, 21, 384)           776448    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 21, 384)           0         \n",
      "                                                                 \n",
      " gru_5 (GRU)                 (None, 384)               887040    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 384)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 385       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4897825 (18.68 MB)\n",
      "Trainable params: 4897825 (18.68 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    print(model[\"name\"])\n",
    "    print(model[\"model\"].summary())\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrainement et Validation des modèles "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Entrainement et validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle bidirectional LSTM\n",
      "Epoch 1/5\n",
      "92/92 [==============================] - 219s 2s/step - loss: 0.4757 - accuracy: 0.7758 - val_loss: 0.4009 - val_accuracy: 0.8251\n",
      "Epoch 2/5\n",
      "92/92 [==============================] - 199s 2s/step - loss: 0.2122 - accuracy: 0.9206 - val_loss: 0.4189 - val_accuracy: 0.8224\n",
      "Epoch 3/5\n",
      "92/92 [==============================] - 212s 2s/step - loss: 0.1105 - accuracy: 0.9617 - val_loss: 0.5576 - val_accuracy: 0.8128\n",
      "Epoch 4/5\n",
      "92/92 [==============================] - 212s 2s/step - loss: 0.0878 - accuracy: 0.9710 - val_loss: 0.7291 - val_accuracy: 0.8074\n",
      "Epoch 5/5\n",
      "92/92 [==============================] - 215s 2s/step - loss: 0.0675 - accuracy: 0.9758 - val_loss: 0.8113 - val_accuracy: 0.7910\n",
      "23/23 [==============================] - 15s 536ms/step\n",
      "Modèle LSTM\n",
      "Epoch 1/5\n",
      "92/92 [==============================] - 85s 853ms/step - loss: 0.4986 - accuracy: 0.7602 - val_loss: 0.3738 - val_accuracy: 0.8333\n",
      "Epoch 2/5\n",
      "92/92 [==============================] - 69s 747ms/step - loss: 0.2224 - accuracy: 0.9155 - val_loss: 0.4094 - val_accuracy: 0.8210\n",
      "Epoch 3/5\n",
      "92/92 [==============================] - 71s 776ms/step - loss: 0.1123 - accuracy: 0.9597 - val_loss: 0.5235 - val_accuracy: 0.8101\n",
      "Epoch 4/5\n",
      "92/92 [==============================] - 73s 794ms/step - loss: 0.0755 - accuracy: 0.9742 - val_loss: 0.5918 - val_accuracy: 0.8046\n",
      "Epoch 5/5\n",
      "92/92 [==============================] - 73s 790ms/step - loss: 0.0550 - accuracy: 0.9809 - val_loss: 0.7665 - val_accuracy: 0.7937\n",
      "23/23 [==============================] - 6s 213ms/step\n",
      "Modèle bidirectional GRU\n",
      "Epoch 1/5\n",
      "92/92 [==============================] - 71s 658ms/step - loss: 0.4569 - accuracy: 0.7840 - val_loss: 0.3868 - val_accuracy: 0.8374\n",
      "Epoch 2/5\n",
      "92/92 [==============================] - 60s 649ms/step - loss: 0.1940 - accuracy: 0.9245 - val_loss: 0.4142 - val_accuracy: 0.8197\n",
      "Epoch 3/5\n",
      "92/92 [==============================] - 62s 671ms/step - loss: 0.0993 - accuracy: 0.9647 - val_loss: 0.4742 - val_accuracy: 0.8183\n",
      "Epoch 4/5\n",
      "92/92 [==============================] - 64s 692ms/step - loss: 0.0702 - accuracy: 0.9773 - val_loss: 0.5680 - val_accuracy: 0.8265\n",
      "Epoch 5/5\n",
      "92/92 [==============================] - 62s 675ms/step - loss: 0.0498 - accuracy: 0.9826 - val_loss: 0.6796 - val_accuracy: 0.8169\n",
      "23/23 [==============================] - 6s 164ms/step\n",
      "Modèle GRU\n",
      "Epoch 1/5\n",
      "92/92 [==============================] - 58s 558ms/step - loss: 0.4582 - accuracy: 0.7879 - val_loss: 0.3757 - val_accuracy: 0.8251\n",
      "Epoch 2/5\n",
      "92/92 [==============================] - 56s 606ms/step - loss: 0.1946 - accuracy: 0.9266 - val_loss: 0.4804 - val_accuracy: 0.8320\n",
      "Epoch 3/5\n",
      "92/92 [==============================] - 49s 531ms/step - loss: 0.1040 - accuracy: 0.9652 - val_loss: 0.5630 - val_accuracy: 0.8210\n",
      "Epoch 4/5\n",
      "92/92 [==============================] - 41s 447ms/step - loss: 0.0733 - accuracy: 0.9735 - val_loss: 0.5616 - val_accuracy: 0.8115\n",
      "Epoch 5/5\n",
      "92/92 [==============================] - 41s 446ms/step - loss: 0.0495 - accuracy: 0.9831 - val_loss: 0.7645 - val_accuracy: 0.8046\n",
      "23/23 [==============================] - 3s 92ms/step\n"
     ]
    }
   ],
   "source": [
    "# entrainement et validation des modèles\n",
    "for model in models:\n",
    "    print(f\"Modèle {model['name']}\")\n",
    "    model['model'].fit(x_train, y_train, validation_data=(x_val, y_val), epochs=5, batch_size=64)\n",
    "    \n",
    "    # Prédire les labels de x_val\n",
    "    y_pred = model['model'].predict(x_val)\n",
    "    \n",
    "    # Arrondir les valeurs\n",
    "    y_pred = np.round(y_pred)\n",
    "    \n",
    "    # calculons l'accuracy\n",
    "    model['score'] = accuracy_score(y_pred, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bidirectional LSTM: 0.7909836065573771\n",
      "LSTM: 0.7937158469945356\n",
      "bidirectional GRU: 0.8169398907103825\n",
      "GRU: 0.8046448087431693\n"
     ]
    }
   ],
   "source": [
    "# Affichons les scores et choisissons le meilleur modèle\n",
    "choosed_model = models[0]\n",
    "for model in models:\n",
    "    print(f\"{model['name']}: {model['score']}\")\n",
    "    if(model['score'] > choosed_model['score']):\n",
    "        choosed_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le modèle choisi est :\n",
      "bidirectional GRU: 0.8169398907103825\n"
     ]
    }
   ],
   "source": [
    "# Affichons le modèle choisi\n",
    "print(\"Le modèle choisi est :\")\n",
    "print(f\"{choosed_model['name']}: {choosed_model['score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\morcodou.seck\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# sauvegarde du modèle\n",
    "choosed_model['model'].save(\"models/english_sentiment_analysis.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
